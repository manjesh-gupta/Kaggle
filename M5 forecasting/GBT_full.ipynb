{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from  datetime import datetime, timedelta\n",
    "import gc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define column types for importing data\n",
    "cal_dtype = {\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n",
    "              \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n",
    "              \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"int8\", 'snap_TX': 'int8', 'snap_WI': 'int8'}\n",
    "price_dtype = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\"}\n",
    "\n",
    "#Read sales data\n",
    "dt = pd.read_csv('sales_train_validation.csv')\n",
    "dt = dt.astype({col: 'int32' for col in dt.select_dtypes('int64').columns})\n",
    "\n",
    "#Read calendar data\n",
    "cal = pd.read_csv('calendar.csv', dtype = cal_dtype)\n",
    "cal['date'] = pd.to_datetime(cal['date'])\n",
    "for col, col_dtype in cal_dtype.items():\n",
    "    if col_dtype == \"category\":\n",
    "        cal[col] = cal[col].cat.codes.astype(\"int16\")\n",
    "        cal[col] -= cal[col].min()\n",
    "cal = cal.astype({col: 'int32' for col in cal.select_dtypes('int64').columns})\n",
    "cal_1 = cal[0:1913]\n",
    "future = cal[0:1941]\n",
    "\n",
    "#Aggregate sales by store_id and dept_it\n",
    "dt_st_dep = dt.groupby(['store_id', 'dept_id'], as_index=False).sum()\n",
    "\n",
    "#Create dataframe to generate proportions for last 28 days to predict individual series from aggregated series\n",
    "dt_y_pred = dt[['id','dept_id','store_id', 'd_1886', 'd_1887', 'd_1888', 'd_1889', 'd_1890', 'd_1891',\n",
    "               'd_1892', 'd_1893', 'd_1894', 'd_1895', 'd_1896', 'd_1897', 'd_1898',\n",
    "               'd_1899', 'd_1900', 'd_1901', 'd_1902', 'd_1903', 'd_1904', 'd_1905', 'd_1906',\n",
    "               'd_1907', 'd_1908', 'd_1909', 'd_1910', 'd_1911', 'd_1912', 'd_1913',]]\n",
    "d_id = dt_y_pred.dept_id.unique()\n",
    "d_id_lst = d_id.tolist()\n",
    "st_id = dt_y_pred.store_id.unique()\n",
    "st_id_lst = st_id.tolist()\n",
    "\n",
    "#Create empty dataframe to store submission\n",
    "df_sub = pd.DataFrame(columns = ['id','dept_id','store_id', 'd_1886', 'd_1887', 'd_1888', 'd_1889', 'd_1890', 'd_1891',\n",
    "               'd_1892', 'd_1893', 'd_1894', 'd_1895', 'd_1896', 'd_1897', 'd_1898',\n",
    "               'd_1899', 'd_1900', 'd_1901', 'd_1902', 'd_1903', 'd_1904', 'd_1905', 'd_1906',\n",
    "               'd_1907', 'd_1908', 'd_1909', 'd_1910', 'd_1911', 'd_1912', 'd_1913',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA_1 HOBBIES_1\n",
      "CA_1 HOBBIES_2\n",
      "CA_1 HOUSEHOLD_1\n",
      "CA_1 HOUSEHOLD_2\n",
      "CA_1 FOODS_1\n",
      "CA_1 FOODS_2\n",
      "CA_1 FOODS_3\n",
      "CA_2 HOBBIES_1\n",
      "CA_2 HOBBIES_2\n",
      "CA_2 HOUSEHOLD_1\n",
      "CA_2 HOUSEHOLD_2\n",
      "CA_2 FOODS_1\n",
      "CA_2 FOODS_2\n",
      "CA_2 FOODS_3\n",
      "CA_3 HOBBIES_1\n",
      "CA_3 HOBBIES_2\n",
      "CA_3 HOUSEHOLD_1\n",
      "CA_3 HOUSEHOLD_2\n",
      "CA_3 FOODS_1\n",
      "CA_3 FOODS_2\n",
      "CA_3 FOODS_3\n",
      "CA_4 HOBBIES_1\n",
      "CA_4 HOBBIES_2\n",
      "CA_4 HOUSEHOLD_1\n",
      "CA_4 HOUSEHOLD_2\n",
      "CA_4 FOODS_1\n",
      "CA_4 FOODS_2\n",
      "CA_4 FOODS_3\n",
      "TX_1 HOBBIES_1\n",
      "TX_1 HOBBIES_2\n",
      "TX_1 HOUSEHOLD_1\n",
      "TX_1 HOUSEHOLD_2\n",
      "TX_1 FOODS_1\n",
      "TX_1 FOODS_2\n",
      "TX_1 FOODS_3\n",
      "TX_2 HOBBIES_1\n",
      "TX_2 HOBBIES_2\n",
      "TX_2 HOUSEHOLD_1\n",
      "TX_2 HOUSEHOLD_2\n",
      "TX_2 FOODS_1\n",
      "TX_2 FOODS_2\n",
      "TX_2 FOODS_3\n",
      "TX_3 HOBBIES_1\n",
      "TX_3 HOBBIES_2\n",
      "TX_3 HOUSEHOLD_1\n",
      "TX_3 HOUSEHOLD_2\n",
      "TX_3 FOODS_1\n",
      "TX_3 FOODS_2\n",
      "TX_3 FOODS_3\n",
      "WI_1 HOBBIES_1\n",
      "WI_1 HOBBIES_2\n",
      "WI_1 HOUSEHOLD_1\n",
      "WI_1 HOUSEHOLD_2\n",
      "WI_1 FOODS_1\n",
      "WI_1 FOODS_2\n",
      "WI_1 FOODS_3\n",
      "WI_2 HOBBIES_1\n",
      "WI_2 HOBBIES_2\n",
      "WI_2 HOUSEHOLD_1\n",
      "WI_2 HOUSEHOLD_2\n",
      "WI_2 FOODS_1\n",
      "WI_2 FOODS_2\n",
      "WI_2 FOODS_3\n",
      "WI_3 HOBBIES_1\n",
      "WI_3 HOBBIES_2\n",
      "WI_3 HOUSEHOLD_1\n",
      "WI_3 HOUSEHOLD_2\n",
      "WI_3 FOODS_1\n",
      "WI_3 FOODS_2\n",
      "WI_3 FOODS_3\n"
     ]
    }
   ],
   "source": [
    "for s_id in st_id_lst:                #Loop over store ids\n",
    "    for d_id in d_id_lst:            #Loop over department ids\n",
    "        print(s_id, d_id)\n",
    "        \n",
    "        #Get all individual series for this aggregate series\n",
    "        hob1_st1 = dt_y_pred.loc[(dt_y_pred['dept_id'] == d_id) & (dt_y_pred['store_id'] == s_id)]\n",
    "        \n",
    "        #Get train set values for specific d_id and s_id\n",
    "        dt_st_dep_n = dt_st_dep.loc[(dt_st_dep['dept_id'] == d_id) & (dt_st_dep['store_id'] == s_id)]\n",
    "        \n",
    "        #Get proportions of individual series in aggregate series\n",
    "        div_coeff_28 = dt_st_dep_n[['store_id','dept_id', 'd_1886', 'd_1887', 'd_1888', 'd_1889', 'd_1890', 'd_1891',\n",
    "                       'd_1892', 'd_1893', 'd_1894', 'd_1895', 'd_1896', 'd_1897', 'd_1898',\n",
    "                       'd_1899', 'd_1900', 'd_1901', 'd_1902', 'd_1903', 'd_1904', 'd_1905', 'd_1906',\n",
    "                       'd_1907', 'd_1908', 'd_1909', 'd_1910', 'd_1911', 'd_1912', 'd_1913',]]\n",
    "        div_coeff_28 = div_coeff_28.reset_index(drop=True)\n",
    "        div_coeff_lst = div_coeff_28.loc[0, :].values.tolist()\n",
    "\n",
    "        hob1_st1['d_1886'] = hob1_st1['d_1886']/div_coeff_lst[2]\n",
    "        hob1_st1['d_1887'] = hob1_st1['d_1887']/div_coeff_lst[3]\n",
    "        hob1_st1['d_1888'] = hob1_st1['d_1888']/div_coeff_lst[4]\n",
    "        hob1_st1['d_1889'] = hob1_st1['d_1889']/div_coeff_lst[5]\n",
    "        hob1_st1['d_1890'] = hob1_st1['d_1890']/div_coeff_lst[6]\n",
    "        hob1_st1['d_1891'] = hob1_st1['d_1891']/div_coeff_lst[7]\n",
    "        hob1_st1['d_1892'] = hob1_st1['d_1892']/div_coeff_lst[8]\n",
    "        hob1_st1['d_1893'] = hob1_st1['d_1893']/div_coeff_lst[9]\n",
    "        hob1_st1['d_1894'] = hob1_st1['d_1894']/div_coeff_lst[10]\n",
    "        hob1_st1['d_1895'] = hob1_st1['d_1895']/div_coeff_lst[11]\n",
    "        hob1_st1['d_1896'] = hob1_st1['d_1896']/div_coeff_lst[12]\n",
    "        hob1_st1['d_1897'] = hob1_st1['d_1897']/div_coeff_lst[13]\n",
    "        hob1_st1['d_1898'] = hob1_st1['d_1898']/div_coeff_lst[14]\n",
    "        hob1_st1['d_1899'] = hob1_st1['d_1899']/div_coeff_lst[15]\n",
    "        hob1_st1['d_1900'] = hob1_st1['d_1900']/div_coeff_lst[16]\n",
    "        hob1_st1['d_1901'] = hob1_st1['d_1901']/div_coeff_lst[17]\n",
    "        hob1_st1['d_1902'] = hob1_st1['d_1902']/div_coeff_lst[18]\n",
    "        hob1_st1['d_1903'] = hob1_st1['d_1903']/div_coeff_lst[19]\n",
    "        hob1_st1['d_1904'] = hob1_st1['d_1904']/div_coeff_lst[20]\n",
    "        hob1_st1['d_1905'] = hob1_st1['d_1905']/div_coeff_lst[21]\n",
    "        hob1_st1['d_1906'] = hob1_st1['d_1906']/div_coeff_lst[22]\n",
    "        hob1_st1['d_1907'] = hob1_st1['d_1907']/div_coeff_lst[23]\n",
    "        hob1_st1['d_1908'] = hob1_st1['d_1908']/div_coeff_lst[24]\n",
    "        hob1_st1['d_1909'] = hob1_st1['d_1909']/div_coeff_lst[25]\n",
    "        hob1_st1['d_1910'] = hob1_st1['d_1910']/div_coeff_lst[26]\n",
    "        hob1_st1['d_1911'] = hob1_st1['d_1911']/div_coeff_lst[27]\n",
    "        hob1_st1['d_1912'] = hob1_st1['d_1912']/div_coeff_lst[28]\n",
    "        hob1_st1['d_1913'] = hob1_st1['d_1913']/div_coeff_lst[29]\n",
    "\n",
    "        #Transform train data\n",
    "        catcols = ['store_id', 'dept_id']\n",
    "        dt_st_dep_n = pd.melt(dt_st_dep_n, \n",
    "                            id_vars = catcols, \n",
    "                            value_vars = [col for col in dt_st_dep_n.columns if col.startswith('d_')], \n",
    "                            var_name = 'd', \n",
    "                            value_name = 'sales')\n",
    "        dt_st_dep_n = dt_st_dep_n.merge(cal, on= \"d\", copy = False)\n",
    "        df_train = dt_st_dep_n\n",
    "\n",
    "        lags = [7, 28]\n",
    "        lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            df_train[lag_col] = df_train['sales'].shift(lag)\n",
    "        wins = [7, 28]\n",
    "        for win in wins :\n",
    "            for lag,lag_col in zip(lags, lag_cols):\n",
    "                df_train[f\"rmean_{lag}_{win}\"] = df_train[lag_col].transform(lambda x : x.rolling(win).mean()) \n",
    "        df_train = df_train.drop(['store_id', 'dept_id', 'd', 'date', 'wm_yr_wk', 'weekday'], axis=1)\n",
    "        df_train.dropna(inplace = True)\n",
    "\n",
    "        y = df_train['sales']\n",
    "        if (s_id.split('_')[0] == 'CA'):\n",
    "            X_train = df_train.drop(['sales', 'snap_TX', 'snap_WI'], axis=1)\n",
    "        elif (s_id.split('_')[0] == 'TX'):\n",
    "            X_train = df_train.drop(['sales', 'snap_CA', 'snap_WI'], axis=1)\n",
    "        else:\n",
    "            X_train = df_train.drop(['sales', 'snap_TX', 'snap_CA'], axis=1)\n",
    "\n",
    "\n",
    "        #Define and fit regressor\n",
    "        gbr = GradientBoostingRegressor()\n",
    "        grid_values_gbr = {'loss': ['ls','lad','huber'],'n_estimators': [50, 100, 250, 500], \n",
    "                           'max_depth':[3, 5, 7], 'learning_rate':[0.05, 0.075, 0.1],\n",
    "                          'criterion': ['mse']}\n",
    "        grid_clf_gbr = GridSearchCV(gbr, param_grid = grid_values_gbr, scoring = 'neg_root_mean_squared_error', cv =5)\n",
    "        grid_clf_gbr.fit(X_train, y)\n",
    "        gbr = grid_clf_gbr.best_estimator_\n",
    "        gbr.fit(X_train,y)\n",
    "\n",
    "        # Preparing dataset for prediction\n",
    "        dt_pred = dt_st_dep.loc[(dt_st_dep['dept_id'] == d_id) & (dt_st_dep['store_id'] == s_id)]\n",
    "        dt_pred = pd.melt(dt_pred, \n",
    "                            id_vars = catcols, \n",
    "                            value_vars = [col for col in dt_pred.columns if col.startswith('d_')], \n",
    "                            var_name = 'd', \n",
    "                            value_name = 'sales')\n",
    "        for i in range(28):\n",
    "            j=i+1914\n",
    "            d = 'd_'+str(j)\n",
    "            app_list = [s_id, d_id, d, -1]\n",
    "            dt_pred.loc[len(dt_pred)] = app_list\n",
    "        dt_pred = dt_pred.merge(future, on= \"d\", copy = False)\n",
    "        dt_pred = dt_pred.drop(['store_id', 'dept_id', 'd', 'date', 'wm_yr_wk', 'weekday'], axis=1)\n",
    "\n",
    "        #Getting features and predicting values for 1st week\n",
    "        lags = [7, 28]\n",
    "        lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            dt_pred[lag_col] = dt_pred['sales'].shift(lag)\n",
    "        wins = [7, 28]\n",
    "        for win in wins :\n",
    "            for lag,lag_col in zip(lags, lag_cols):\n",
    "                dt_pred[f\"rmean_{lag}_{win}\"] = dt_pred[lag_col].transform(lambda x : x.rolling(win).mean()) \n",
    "        if (s_id.split('_')[0] == 'CA'):\n",
    "            X_test_tmp = dt_pred.drop(['sales', 'snap_TX', 'snap_WI'], axis=1)\n",
    "        elif (s_id.split('_')[0] == 'TX'):\n",
    "            X_test_tmp = dt_pred.drop(['sales', 'snap_CA', 'snap_WI'], axis=1)\n",
    "        else:\n",
    "            X_test_tmp = dt_pred.drop(['sales', 'snap_TX', 'snap_CA'], axis=1)\n",
    "        X_test_curr = X_test_tmp[1913:1920]\n",
    "        y_pred_curr = gbr.predict(X_test_curr)\n",
    "        y_pred_curr_lst = list(y_pred_curr)\n",
    "\n",
    "        #Setting values for 1st week\n",
    "        dt_pred.loc[1913, 'sales']=y_pred_curr_lst[0]\n",
    "        dt_pred.loc[1914, 'sales']=y_pred_curr_lst[1]\n",
    "        dt_pred.loc[1915, 'sales']=y_pred_curr_lst[2]\n",
    "        dt_pred.loc[1916, 'sales']=y_pred_curr_lst[3]\n",
    "        dt_pred.loc[1917, 'sales']=y_pred_curr_lst[4]\n",
    "        dt_pred.loc[1918, 'sales']=y_pred_curr_lst[5]\n",
    "        dt_pred.loc[1919, 'sales']=y_pred_curr_lst[6]\n",
    "\n",
    "        #Getting features and predicting values for 2nd week\n",
    "        lags = [7, 28]\n",
    "        lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            dt_pred[lag_col] = dt_pred['sales'].shift(lag)\n",
    "        wins = [7, 28]\n",
    "        for win in wins :\n",
    "            for lag,lag_col in zip(lags, lag_cols):\n",
    "                dt_pred[f\"rmean_{lag}_{win}\"] = dt_pred[lag_col].transform(lambda x : x.rolling(win).mean()) \n",
    "        if (s_id.split('_')[0] == 'CA'):\n",
    "            X_test_tmp = dt_pred.drop(['sales', 'snap_TX', 'snap_WI'], axis=1)\n",
    "        elif (s_id.split('_')[0] == 'TX'):\n",
    "            X_test_tmp = dt_pred.drop(['sales', 'snap_CA', 'snap_WI'], axis=1)\n",
    "        else:\n",
    "            X_test_tmp = dt_pred.drop(['sales', 'snap_TX', 'snap_CA'], axis=1)\n",
    "        X_test_curr = X_test_tmp[1920:1927]\n",
    "        y_pred_curr = gbr.predict(X_test_curr)\n",
    "        y_pred_curr_lst = list(y_pred_curr)\n",
    "\n",
    "        #Setting values for 2nd week\n",
    "        dt_pred.loc[1920, 'sales']=y_pred_curr_lst[0]\n",
    "        dt_pred.loc[1921, 'sales']=y_pred_curr_lst[1]\n",
    "        dt_pred.loc[1922, 'sales']=y_pred_curr_lst[2]\n",
    "        dt_pred.loc[1923, 'sales']=y_pred_curr_lst[3]\n",
    "        dt_pred.loc[1924, 'sales']=y_pred_curr_lst[4]\n",
    "        dt_pred.loc[1925, 'sales']=y_pred_curr_lst[5]\n",
    "        dt_pred.loc[1926, 'sales']=y_pred_curr_lst[6]\n",
    "\n",
    "        # Getting features and predicting values for 3rd week\n",
    "        lags = [7, 28]\n",
    "        lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            dt_pred[lag_col] = dt_pred['sales'].shift(lag)\n",
    "        wins = [7, 28]\n",
    "        for win in wins :\n",
    "            for lag,lag_col in zip(lags, lag_cols):\n",
    "                dt_pred[f\"rmean_{lag}_{win}\"] = dt_pred[lag_col].transform(lambda x : x.rolling(win).mean()) \n",
    "        if (s_id.split('_')[0] == 'CA'):\n",
    "            X_test_tmp = dt_pred.drop(['sales', 'snap_TX', 'snap_WI'], axis=1)\n",
    "        elif (s_id.split('_')[0] == 'TX'):\n",
    "            X_test_tmp = dt_pred.drop(['sales', 'snap_CA', 'snap_WI'], axis=1)\n",
    "        else:\n",
    "            X_test_tmp = dt_pred.drop(['sales', 'snap_TX', 'snap_CA'], axis=1)\n",
    "        X_test_curr = X_test_tmp[1927:1934]\n",
    "        y_pred_curr = gbr.predict(X_test_curr)\n",
    "        y_pred_curr_lst = list(y_pred_curr)\n",
    "\n",
    "        # Setting values for 3rd week\n",
    "        dt_pred.loc[1927, 'sales']=y_pred_curr_lst[0]\n",
    "        dt_pred.loc[1928, 'sales']=y_pred_curr_lst[1]\n",
    "        dt_pred.loc[1929, 'sales']=y_pred_curr_lst[2]\n",
    "        dt_pred.loc[1930, 'sales']=y_pred_curr_lst[3]\n",
    "        dt_pred.loc[1931, 'sales']=y_pred_curr_lst[4]\n",
    "        dt_pred.loc[1932, 'sales']=y_pred_curr_lst[5]\n",
    "        dt_pred.loc[1933, 'sales']=y_pred_curr_lst[6]\n",
    "\n",
    "        # Getting features and predicting values for 4th week\n",
    "        lags = [7, 28]\n",
    "        lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
    "        for lag, lag_col in zip(lags, lag_cols):\n",
    "            dt_pred[lag_col] = dt_pred['sales'].shift(lag)\n",
    "        wins = [7, 28]\n",
    "        for win in wins :\n",
    "            for lag,lag_col in zip(lags, lag_cols):\n",
    "                dt_pred[f\"rmean_{lag}_{win}\"] = dt_pred[lag_col].transform(lambda x : x.rolling(win).mean()) \n",
    "        if (s_id.split('_')[0] == 'CA'):\n",
    "            X_test_tmp = dt_pred.drop(['sales', 'snap_TX', 'snap_WI'], axis=1)\n",
    "        elif (s_id.split('_')[0] == 'TX'):\n",
    "            X_test_tmp = dt_pred.drop(['sales', 'snap_CA', 'snap_WI'], axis=1)\n",
    "        else:\n",
    "            X_test_tmp = dt_pred.drop(['sales', 'snap_TX', 'snap_CA'], axis=1)\n",
    "        X_test_curr = X_test_tmp[1934:1941]\n",
    "        y_pred_curr = gbr.predict(X_test_curr)\n",
    "        y_pred_curr_lst = list(y_pred_curr)\n",
    "\n",
    "        #Setting the predicted values for 4th week\n",
    "        dt_pred.loc[1934, 'sales']=y_pred_curr_lst[0]\n",
    "        dt_pred.loc[1935, 'sales']=y_pred_curr_lst[1]\n",
    "        dt_pred.loc[1936, 'sales']=y_pred_curr_lst[2]\n",
    "        dt_pred.loc[1937, 'sales']=y_pred_curr_lst[3]\n",
    "        dt_pred.loc[1938, 'sales']=y_pred_curr_lst[4]\n",
    "        dt_pred.loc[1939, 'sales']=y_pred_curr_lst[5]\n",
    "        dt_pred.loc[1940, 'sales']=y_pred_curr_lst[6]\n",
    "\n",
    "        #Plotting predicted and actual values\n",
    "        dt_pred_temp = dt_pred.tail(28)\n",
    "        y_pred_lst = dt_pred_temp['sales'].values.tolist()\n",
    "        \n",
    "        #Generating individual series predictions based on aggregated predictions and historical proportions\n",
    "        hob1_st1['d_1886'] = hob1_st1['d_1886']*y_pred_lst[0]\n",
    "        hob1_st1['d_1887'] = hob1_st1['d_1887']*y_pred_lst[1]\n",
    "        hob1_st1['d_1888'] = hob1_st1['d_1888']*y_pred_lst[2]\n",
    "        hob1_st1['d_1889'] = hob1_st1['d_1889']*y_pred_lst[3]\n",
    "        hob1_st1['d_1890'] = hob1_st1['d_1890']*y_pred_lst[4]\n",
    "        hob1_st1['d_1891'] = hob1_st1['d_1891']*y_pred_lst[5]\n",
    "        hob1_st1['d_1892'] = hob1_st1['d_1892']*y_pred_lst[6]\n",
    "        hob1_st1['d_1893'] = hob1_st1['d_1893']*y_pred_lst[7]\n",
    "        hob1_st1['d_1894'] = hob1_st1['d_1894']*y_pred_lst[8]\n",
    "        hob1_st1['d_1895'] = hob1_st1['d_1895']*y_pred_lst[9]\n",
    "        hob1_st1['d_1896'] = hob1_st1['d_1896']*y_pred_lst[10]\n",
    "        hob1_st1['d_1897'] = hob1_st1['d_1897']*y_pred_lst[11]\n",
    "        hob1_st1['d_1898'] = hob1_st1['d_1898']*y_pred_lst[12]\n",
    "        hob1_st1['d_1899'] = hob1_st1['d_1899']*y_pred_lst[13]\n",
    "        hob1_st1['d_1900'] = hob1_st1['d_1900']*y_pred_lst[14]\n",
    "        hob1_st1['d_1901'] = hob1_st1['d_1901']*y_pred_lst[15]\n",
    "        hob1_st1['d_1902'] = hob1_st1['d_1902']*y_pred_lst[16]\n",
    "        hob1_st1['d_1903'] = hob1_st1['d_1903']*y_pred_lst[17]\n",
    "        hob1_st1['d_1904'] = hob1_st1['d_1904']*y_pred_lst[18]\n",
    "        hob1_st1['d_1905'] = hob1_st1['d_1905']*y_pred_lst[19]\n",
    "        hob1_st1['d_1906'] = hob1_st1['d_1906']*y_pred_lst[20]\n",
    "        hob1_st1['d_1907'] = hob1_st1['d_1907']*y_pred_lst[21]\n",
    "        hob1_st1['d_1908'] = hob1_st1['d_1908']*y_pred_lst[22]\n",
    "        hob1_st1['d_1909'] = hob1_st1['d_1909']*y_pred_lst[23]\n",
    "        hob1_st1['d_1910'] = hob1_st1['d_1910']*y_pred_lst[24]\n",
    "        hob1_st1['d_1911'] = hob1_st1['d_1911']*y_pred_lst[25]\n",
    "        hob1_st1['d_1912'] = hob1_st1['d_1912']*y_pred_lst[26]\n",
    "        hob1_st1['d_1913'] = hob1_st1['d_1913']*y_pred_lst[27]\n",
    "        \n",
    "        #Adding current predictions to submission file\n",
    "        df_sub = pd.concat([df_sub, hob1_st1], axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30490 entries, 0 to 30489\n",
      "Data columns (total 31 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        30490 non-null  object \n",
      " 1   dept_id   30490 non-null  object \n",
      " 2   store_id  30490 non-null  object \n",
      " 3   d_1886    30490 non-null  float64\n",
      " 4   d_1887    30490 non-null  float64\n",
      " 5   d_1888    30490 non-null  float64\n",
      " 6   d_1889    30490 non-null  float64\n",
      " 7   d_1890    30490 non-null  float64\n",
      " 8   d_1891    30490 non-null  float64\n",
      " 9   d_1892    30490 non-null  float64\n",
      " 10  d_1893    30490 non-null  float64\n",
      " 11  d_1894    30490 non-null  float64\n",
      " 12  d_1895    30490 non-null  float64\n",
      " 13  d_1896    30490 non-null  float64\n",
      " 14  d_1897    30490 non-null  float64\n",
      " 15  d_1898    30490 non-null  float64\n",
      " 16  d_1899    30490 non-null  float64\n",
      " 17  d_1900    30490 non-null  float64\n",
      " 18  d_1901    30490 non-null  float64\n",
      " 19  d_1902    30490 non-null  float64\n",
      " 20  d_1903    30490 non-null  float64\n",
      " 21  d_1904    30490 non-null  float64\n",
      " 22  d_1905    30490 non-null  float64\n",
      " 23  d_1906    30490 non-null  float64\n",
      " 24  d_1907    30490 non-null  float64\n",
      " 25  d_1908    30490 non-null  float64\n",
      " 26  d_1909    30490 non-null  float64\n",
      " 27  d_1910    30490 non-null  float64\n",
      " 28  d_1911    30490 non-null  float64\n",
      " 29  d_1912    30490 non-null  float64\n",
      " 30  d_1913    30490 non-null  float64\n",
      "dtypes: float64(28), object(3)\n",
      "memory usage: 7.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_sub.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_2 = df_sub.drop(['store_id', 'dept_id'], axis=1)  \n",
    "COLUMN_NAMES = ['id', 'F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15','F16','F17','F18','F19',\n",
    "               'F20', 'F21','F22','F23','F24','F25','F26','F27','F28']\n",
    "df_sub_2.columns = COLUMN_NAMES\n",
    "df_sub_c = df_sub_2.copy()\n",
    "df_sub_c[\"id\"] = df_sub_c[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "df_sub_2 = pd.concat([df_sub_2, df_sub_c], axis=0, sort=False)\n",
    "df_sub_2.to_csv(\"submission_gbt_grid.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60980, 30490)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub_2.id.nunique(), df_sub_2[\"id\"].str.contains(\"validation$\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60980, 29)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub_2.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
